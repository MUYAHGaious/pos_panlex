{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968be7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done. File saved with up to 100 words per POS: english_words_by_pos.json\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import treebank\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "\n",
    "wordnet_pos_map = {\n",
    "    'NOUN': wn.NOUN,\n",
    "    'VERB': wn.VERB,\n",
    "    'ADJ': wn.ADJ,\n",
    "    'ADV': wn.ADV\n",
    "}\n",
    "\n",
    "ud_pos_list = ['DET', 'PRON', 'ADP', 'CONJ', 'NUM', 'INTJ']\n",
    "\n",
    "wordnet_words = defaultdict(set)\n",
    "limit_per_pos = 100\n",
    "\n",
    "for pos_name, wn_pos in wordnet_pos_map.items():\n",
    "    for synset in wn.all_synsets(wn_pos):\n",
    "        for lemma in synset.lemmas():\n",
    "            word = lemma.name().replace(\"_\", \" \").lower()\n",
    "            if word.isalpha():\n",
    "                wordnet_words[pos_name].add(word)\n",
    "        if len(wordnet_words[pos_name]) >= limit_per_pos * 2:\n",
    "            break  \n",
    "\n",
    "treebank_words = defaultdict(set)\n",
    "tagged_sents = treebank.tagged_sents(tagset='universal')\n",
    "\n",
    "for sent in tagged_sents:\n",
    "    for word, tag in sent:\n",
    "        tag = tag.upper()\n",
    "        if tag in ud_pos_list and word.isalpha():\n",
    "            treebank_words[tag].add(word.lower())\n",
    "    if all(len(treebank_words[t]) >= limit_per_pos * 2 for t in ud_pos_list):\n",
    "        break\n",
    "\n",
    "combined_pos_words = {}\n",
    "\n",
    "for pos in wordnet_words:\n",
    "    words = list(wordnet_words[pos])\n",
    "    sample_size = min(limit_per_pos, len(words))\n",
    "    combined_pos_words[pos] = sorted(random.sample(words, sample_size))\n",
    "\n",
    "for pos in ud_pos_list:\n",
    "    words = list(treebank_words[pos])\n",
    "    sample_size = min(limit_per_pos, len(words))\n",
    "    combined_pos_words[pos] = sorted(random.sample(words, sample_size))\n",
    "\n",
    "#Save to JSON\n",
    "with open(\"english_words_by_pos.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(combined_pos_words, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Done. File saved with up to {limit_per_pos} words per POS: english_words_by_pos.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpip install transformers sentencepiece --quiet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece --quiet\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(\"english_words_by_pos.json\", encoding=\"utf-8\") as f:\n",
    "    english_words = json.load(f)\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "lang_code_map = {\n",
    "    \"FRA\": \"fra_Latn\",\n",
    "    \"BAM\": \"bam_Latn\",\n",
    "    \"WOL\": \"wol_Latn\"\n",
    "}\n",
    "\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer, src_lang=\"eng_Latn\", device=device)\n",
    "\n",
    "def translate_batch(batch, tgt_lang):\n",
    "    results = []\n",
    "    try:\n",
    "        translations = translator(batch, tgt_lang=tgt_lang, max_length=128)\n",
    "        for t in translations:\n",
    "            results.append(t['translation_text'])\n",
    "    except Exception:\n",
    "        results = [\"NA\"] * len(batch)\n",
    "    return results\n",
    "\n",
    "final_blocks = []\n",
    "\n",
    "for pos, words in english_words.items():\n",
    "    print(f\"\\nüîµ Translating POS: {pos}\")\n",
    "    df = pd.DataFrame({\"ENG\": words})\n",
    "    \n",
    "    for lang_code, tgt_lang in lang_code_map.items():\n",
    "        translations = []\n",
    "        for i in tqdm(range(0, len(words), 8), desc=f\"{pos} ‚Üí {lang_code}\"):\n",
    "            batch = words[i:i+8]\n",
    "            translated = translate_batch(batch, tgt_lang)\n",
    "            translations.extend(translated)\n",
    "        df[lang_code] = translations\n",
    "    \n",
    "    df.columns = [f\"{col}_{pos}\" if col != \"ENG\" else f\"ENG_{pos}\" for col in df.columns]\n",
    "    final_blocks.append(df)\n",
    "\n",
    "#export\n",
    "final_df = pd.concat(final_blocks, axis=1)\n",
    "final_df.to_csv(\"multilingual_pos_dataset.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\n‚úÖ Final CSV saved: multilingual_pos_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438f51d",
   "metadata": {},
   "source": [
    "\n",
    "````markdown\n",
    "# üåç Multilingual POS-Aligned Dataset Generator\n",
    "\n",
    "This project builds a multilingual dataset with words aligned by **Part of Speech (POS)** across English, French, Bambara, and Wolof. It leverages:\n",
    "- **NLTK's WordNet and Treebank** for curated English words by POS\n",
    "- **Meta's NLLB (No Language Left Behind)** model for translation\n",
    "- **Transformers (ü§ó Hugging Face)** for running the translation pipeline\n",
    "- Outputs: A structured CSV with English words translated into multiple languages, grouped by POS (e.g., NOUN, VERB, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Output Example\n",
    "\n",
    "| ENG_NOUN       | FRA_NOUN   | BAM_NOUN | WOL_NOUN | ENG_VERB | FRA_VERB | BAM_VERB | WOL_VERB |\n",
    "|----------------|------------|----------|----------|-----------|-----------|-----------|-----------|\n",
    "| abduction      | abduction  | NA       | NA       | accept    | accepter | NA        | NA        |\n",
    "| accident       | accident   | NA       | NA       | add       | ajouter  | NA        | NA        |\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Project Structure\n",
    "\n",
    "```bash\n",
    ".\n",
    "‚îú‚îÄ‚îÄ english_words_by_pos.json      # Extracted English words by POS\n",
    "‚îú‚îÄ‚îÄ multilingual_translations.csv  # Final structured CSV output\n",
    "‚îú‚îÄ‚îÄ extract_english_words.py       # Collects words from WordNet & Treebank\n",
    "‚îú‚îÄ‚îÄ translate_with_nllb.py         # Translates words using Meta's NLLB\n",
    "‚îî‚îÄ‚îÄ README.md                      # This file\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ How It Works\n",
    "\n",
    "### Step 1: Extract English Words by POS\n",
    "\n",
    "Using:\n",
    "\n",
    "* **WordNet** (for NOUN, VERB, ADJ, ADV)\n",
    "* **Treebank corpus** (for DET, PRON, ADP, CONJ, NUM, INTJ)\n",
    "\n",
    "The output is saved to a JSON file:\n",
    "\n",
    "```bash\n",
    "english_words_by_pos.json\n",
    "```\n",
    "\n",
    "### Step 2: Translate with NLLB\n",
    "\n",
    "The script loads the English POS dictionary and uses the NLLB model to translate each word into:\n",
    "\n",
    "* **French (`fra_Latn`)**\n",
    "* **Bambara (`bam_Latn`)**\n",
    "* **Wolof (`wol_Latn`)**\n",
    "\n",
    "Progress is saved to:\n",
    "\n",
    "```bash\n",
    "multilingual_translations.csv\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† Installation\n",
    "\n",
    "Make sure you're using **Google Colab** or a compatible GPU machine.\n",
    "\n",
    "```python\n",
    "# Inside a notebook cell:\n",
    "!pip install transformers sentencepiece\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Usage Instructions\n",
    "\n",
    "1. **Extract English Words**\n",
    "\n",
    "```bash\n",
    "python extract_english_words.py\n",
    "```\n",
    "\n",
    "2. **Run Translation**\n",
    "\n",
    "```bash\n",
    "python translate_with_nllb.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "\n",
    "* Words are randomly sampled (up to 500 per POS).\n",
    "* If a translation fails, it returns `NA`.\n",
    "* The NLLB model used: `facebook/nllb-200-distilled-600M`.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÆ Future Improvements\n",
    "\n",
    "* Add fallback translation strategies (e.g., synonyms, PanLex API)\n",
    "* Include more languages or dialects\n",
    "* Clean and filter translations using confidence scoring\n",
    "\n",
    "---\n",
    "\n",
    "## üë§ Authors\n",
    "\n",
    "- [Muyah Gaious](https://github.com/MUYAHGaious) - Software Developer with expertise in AI and backend systems.\n",
    "- [Nichoh Elmic](https://github.com/Nichoh-Elmic) - Passionate coder focused on web technologies and DevOps.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae393b6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
